{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfZoSVGoIAFX"
   },
   "source": [
    "# CSCI 360 Project 4: Machine Learning\n",
    "## Date Due: 11:59 PM 4/30/2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpmNtjL2IAFf"
   },
   "source": [
    "###### Name: \n",
    "###### Student ID: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Google Colab and the Anaconda distribution. In this project, you will work with different datasets and technologies to simulate the data science pipeline and help teach machine learning algorithms. The project is broken into 4 distinct parts, each which do not overlap and do not require previous parts to be done. The first part focuses on the K-nearest neighbors algorithm, which is built completely from numpy alone: this algorithm is run on the iris dataset from UCI's ML repository. The second part focuses on ensembles as a way to learn practical applications of concepts learned in class such as Linear Perceptrons, Neural Networks, Decision Trees, and other machine learning algorithms. You will build 3 different ensembles using the SKlearn package, run against the breast cancer dataset from UCI's ML repository. The third part focuses on Convolutional Neural Networks built from Pytorch: you will build a nn.Sequential model run against the Cifar-10 dataset for image classification. This portion briefly touches on the concepts of neural networks and showcases recent technologies directed towards the intersection of Computer Vision and Machine Learning. The fourth part focuses on Recurrent Neural Networks again built from Pytorch. This time, you will build the neural network through a nn.Module class and run it against the AG News dataset for text classification. This part exemplifies on neural networks and natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that we are running everything via Google Colab. Why not download and run everything on Anaconda distribution and do the project locally? The problem with doing so is that your graphics card may not be as good for parts 3 and 4. With Google Colab, we can set the runtime environment (using the Runtime -> Change Runtime Type setting above) which will significantly increase the speed of training our machine learning models. Set the runtime to GPU in order to fully utilize Google Colab. We use Google's Tesla K80 GPU, which is significantly more powerful than your integrated graphics cards on your laptops or desktops. Google Colab also already comes with all packages involved in this project, including Pytorch and Sklearn, saving the headache that is installation of Anaconda and the packages involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqan-eRCIAFf"
   },
   "source": [
    "### Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These few codeblocks are here to simply important necessary packages for the entire project. Numpy and Matplotlib are staples of the data science pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 455,
     "status": "ok",
     "timestamp": 1617343178371,
     "user": {
      "displayName": "Michael Yuen",
      "photoUrl": "",
      "userId": "13545254429662867295"
     },
     "user_tz": 420
    },
    "id": "H-ZwP7D7IAFg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the importing of the iris dataset. Make sure the google drive and folder is set properly in order to make sure the project works on your Google Colab setup and you can pull the dataset properly. Do copy and paste the authorization code to give access of your Google Drive to Google Colab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iris dataset is a toy dataset with 150 data points, each with 4 features and a class. The data is holds the following features:\n",
    "\n",
    "1. sepal length in cm\n",
    "2. sepal width in cm\n",
    "3. petal length in cm\n",
    "4. petal width in cm\n",
    "5. class: Iris Setosa (0.0), Iris Versicolour (1.0), Iris Virginica (2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22026,
     "status": "ok",
     "timestamp": 1617343199952,
     "user": {
      "displayName": "Michael Yuen",
      "photoUrl": "",
      "userId": "13545254429662867295"
     },
     "user_tz": 420
    },
    "id": "NwebkJgoIAFh",
    "outputId": "447796a0-1901-47ca-820f-a187b6dd3c5e"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    iris = np.genfromtxt(\"/content/drive/My Drive/CSCI 360 Spring 2021/datasets/iris-dataset/iris.data\", delimiter=\",\", dtype=None)\n",
    "except:\n",
    "    iris = np.genfromtxt(\"./datasets/iris-dataset/iris.data\", delimiter=\",\", dtype=None)\n",
    "X = iris[:,0:-1] #features are the other columns\n",
    "Y = iris[:,-1] #target value (iris specimen) is last column\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally it is better to do a train, validation, and test dataset. Evaluation generally increases that way. However, for the purposes of this particular question, we only split the data into a training and testing set that isn't randomized.\n",
    "\n",
    "Also shown below are a couple of plots showing the relations between the first and second features based on class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "executionInfo": {
     "elapsed": 21240,
     "status": "ok",
     "timestamp": 1617343200236,
     "user": {
      "displayName": "Michael Yuen",
      "photoUrl": "",
      "userId": "13545254429662867295"
     },
     "user_tz": 420
    },
    "id": "iWVB3ky23FEk",
    "outputId": "07e9461b-f8a6-4271-dc57-530c9137283f"
   },
   "outputs": [],
   "source": [
    "def split_data(features, targets):\n",
    "  x_dim, y_dim = np.atleast_2d(features).shape\n",
    "  splitting_point = int(round(0.75 * x_dim)) #75% of the data\n",
    "  X_train, X_test = features[:splitting_point,:],features[splitting_point:,:]\n",
    "  Y = np.asarray(targets).flatten()\n",
    "  Y_train, Y_test = Y[:splitting_point],Y[splitting_point:]\n",
    "  return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "#Normally, good practice is to randomize and crossvalidate the data, but for grading purposes, don't do it.\n",
    "np.random.seed(0) #do not change this line\n",
    "X_train, Y_train, X_test, Y_test = split_data(X, Y)\n",
    "print(X_test.shape)\n",
    "print(X_train.shape)\n",
    "print(Y_test.shape)\n",
    "print(Y_train.shape)\n",
    "plt.scatter(x=X_train[:,0:1], y=X_train[:,1:2], c=Y_train.reshape(X_train[:,0:1].shape))\n",
    "plt.show()\n",
    "plt.scatter(x=X_test[:,0:1], y=X_test[:,1:2], c=Y_test.reshape(X_test[:,0:1].shape))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6mQxj7TIAFi"
   },
   "source": [
    "## Question 1) k-Nearest Neighbors - 5 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will create a key portion to a kNN classifier, the prediction function from scratch without using sklearn or other packages besides numpy. Do **NOT** import additional packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the initial blocks, the data preprocessing is done. The basic layout of the kNN class is done below; the only thing to figure out is the prediction function. The prediction function should follow a general algorithm:\n",
    "\n",
    "```\n",
    "For each datapoint in the test data:\n",
    "    Calculate the distance between training and testing data using self.distance\n",
    "    Sort the distances by the K nearest neighbors (self.K, the given parameter)\n",
    "    Calculate the class per datapoint through the neighbors found\n",
    "Return a set of predicted classes per datapoint in the test data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 515,
     "status": "ok",
     "timestamp": 1617343200754,
     "user": {
      "displayName": "Michael Yuen",
      "photoUrl": "",
      "userId": "13545254429662867295"
     },
     "user_tz": 420
    },
    "id": "xyqzMJ-FIAFj"
   },
   "outputs": [],
   "source": [
    "class kNN_classifier:\n",
    "    def __init__(self, X, Y, K=1):\n",
    "        self.train(X, Y, K)\n",
    "\n",
    "    def train(self, X, Y, K=1):\n",
    "        self.X = np.asarray(X)\n",
    "        self.Y = np.asarray(Y)\n",
    "        self.K = K\n",
    "        self.classes = np.unique(Y)\n",
    "\n",
    "    def predict(self, Xtest):\n",
    "        '''Implement this function'''\n",
    "        #pass\n",
    "\n",
    "    def evaluate(self, Ypred, Ytest):\n",
    "        accuracy = 0\n",
    "        for predicted, true_val in zip(Ypred, Ytest):\n",
    "            if predicted == true_val:\n",
    "                accuracy += 1\n",
    "        return accuracy / Ytest.shape[0]\n",
    "  \n",
    "    def distance(self, X, Y):\n",
    "        return np.sqrt(np.sum((X-Y)**2, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation function below copied, as we will use this later on separately in both parts 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(Ypred, Ytest):\n",
    "    accuracy = 0\n",
    "    for predicted, true_val in zip(Ypred, Ytest):\n",
    "        if predicted == true_val:\n",
    "            accuracy += 1\n",
    "    return accuracy / Ytest.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run some general test cases with your kNN classifier below. You can compare your results to the SKlearn implementation of kNN (which does not have to be exact, but needs to be relatively close). Since the dataset is small, the kNN results should slowly become worse and worse on this particular dataset with more neighbors. Optimally, you do something called parameter tuning by finding a parameter with decent training and validation accuracy that gets the best possible testing accuracy every time. Therefore you want to avoid overfitting or underfitting to the data (e.g. K=1 is probably overfitting, and K=50 is probably underfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 506,
     "status": "ok",
     "timestamp": 1617343200756,
     "user": {
      "displayName": "Michael Yuen",
      "photoUrl": "",
      "userId": "13545254429662867295"
     },
     "user_tz": 420
    },
    "id": "OO279uxBfWlT",
    "outputId": "2045c057-8516-4891-c850-9d15288e2385"
   },
   "outputs": [],
   "source": [
    "kNN = kNN_classifier(X_train, Y_train, 1)\n",
    "pred = kNN.predict(X_test)\n",
    "eval_results = kNN.evaluate(pred, Y_test)\n",
    "print(eval_results)\n",
    "assert eval_results == 1.0\n",
    "\n",
    "kNN = kNN_classifier(X_train, Y_train, 5)\n",
    "pred = kNN.predict(X_test)\n",
    "eval_results = kNN.evaluate(pred, Y_test)\n",
    "print(eval_results)\n",
    "assert eval_results >= 0.97\n",
    "\n",
    "kNN = kNN_classifier(X_train, Y_train, 10)\n",
    "pred = kNN.predict(X_test)\n",
    "eval_results = kNN.evaluate(pred, Y_test)\n",
    "print(eval_results)\n",
    "assert eval_results >= 0.90\n",
    "\n",
    "kNN = kNN_classifier(X_train, Y_train, 35)\n",
    "pred = kNN.predict(X_test)\n",
    "eval_results = kNN.evaluate(pred, Y_test)\n",
    "print(eval_results)\n",
    "assert eval_results >= 0.85\n",
    "\n",
    "kNN = kNN_classifier(X_train, Y_train, 50)\n",
    "pred = kNN.predict(X_test)\n",
    "eval_results = kNN.evaluate(pred, Y_test)\n",
    "print(eval_results)\n",
    "assert eval_results >= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your results to the K-nearest neighbors implementation from the sklearn package. The results do not need to be exact, but it can be a decent starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, a side note about sklearn: you can find documentation here (more importantly used in part 2). https://scikit-learn.org/stable/user_guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1690,
     "status": "ok",
     "timestamp": 1617343201947,
     "user": {
      "displayName": "Michael Yuen",
      "photoUrl": "",
      "userId": "13545254429662867295"
     },
     "user_tz": 420
    },
    "id": "aN7AnuoYOnVb",
    "outputId": "5fb7912f-341b-4485-8035-740058953089"
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "skkNN = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "skkNN_fit = skkNN.fit(X_train, Y_train)\n",
    "skkNN_pred = skkNN.predict(X_test)\n",
    "eval_results = evaluate(skkNN_pred, Y_test)\n",
    "print(eval_results)\n",
    "\n",
    "skkNN = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "skkNN_fit = skkNN.fit(X_train, Y_train)\n",
    "skkNN_pred = skkNN.predict(X_test)\n",
    "eval_results = evaluate(skkNN_pred, Y_test)\n",
    "print(eval_results)\n",
    "\n",
    "skkNN = neighbors.KNeighborsClassifier(n_neighbors=10)\n",
    "skkNN_fit = skkNN.fit(X_train, Y_train)\n",
    "skkNN_pred = skkNN.predict(X_test)\n",
    "eval_results = evaluate(skkNN_pred, Y_test)\n",
    "print(eval_results)\n",
    "\n",
    "skkNN = neighbors.KNeighborsClassifier(n_neighbors=35)\n",
    "skkNN_fit = skkNN.fit(X_train, Y_train)\n",
    "skkNN_pred = skkNN.predict(X_test)\n",
    "eval_results = evaluate(skkNN_pred, Y_test)\n",
    "print(eval_results)\n",
    "\n",
    "skkNN = neighbors.KNeighborsClassifier(n_neighbors=50)\n",
    "skkNN_fit = skkNN.fit(X_train, Y_train)\n",
    "skkNN_pred = skkNN.predict(X_test)\n",
    "eval_results = evaluate(skkNN_pred, Y_test)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "198789f00350b55a7d43d1bebc5ea2e0",
     "grade": true,
     "grade_id": "589jyi34h",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea74f97d397a0b730f9570e396145e2a",
     "grade": true,
     "grade_id": "bhjritu46890mno5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c42fa3585d54ff48aefdd302aac3dc1d",
     "grade": true,
     "grade_id": "0459rt6jkiuwoe",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ea81a50c15235278a2116dc822f1164",
     "grade": true,
     "grade_id": "40jw8i6bhet",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d14587c435a50f01463631b12e31737a",
     "grade": true,
     "grade_id": "4890jweihsbm0e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulOwwGw8IAFj"
   },
   "source": [
    "## Question 2) Ensemble Learning - 6 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will create 3 different ensembles to simulate ensemble learning. The topic at hand is a bit different from what you have learned within lecture, but is a good way to practice and see the concepts learned within lecture in a \"grab-bag\" format: you have learned different concepts such as linear perceptrons, naive bayes, and neural networks, now you can put these to use through sklearn's packages without having to manually build them up from scratch. You can then pull different implementations, parameters, and classifiers to run against the breast cancer dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of an ensemble in machine learning is to take multiple machine learning algorithms and combine them to make a stronger model together. For example, a linear perceptron is not strong enough alone on a dataset, only leading to mediocre accuracy. However, combining the linear perceptron with a decision tree will make the accuracy significantly better. That is the power of an ensemble classifier, and in essence this part (and this project) is an ensemble of different methods used in Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is again another data preprocessing block. This time there are 9 features and significantly more data. \n",
    "\n",
    "0. (Ignore the very first feature, as this is just id numbers)\n",
    "1. Clump Thickness: 1 - 10\n",
    "2. Uniformity of Cell Size: 1 - 10\n",
    "3. Uniformity of Cell Shape: 1 - 10\n",
    "4. Marginal Adhesion: 1 - 10\n",
    "5. Single Epithelial Cell Size: 1 - 10\n",
    "6. Bare Nuclei: 1 - 10\n",
    "7. Bland Chromatin: 1 - 10\n",
    "8. Normal Nucleoli: 1 - 10\n",
    "9. Mitoses: 1 - 10\n",
    "10. Class: (2.0 for benign, 4.0 for malignant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 959,
     "status": "ok",
     "timestamp": 1617343201948,
     "user": {
      "displayName": "Michael Yuen",
      "photoUrl": "",
      "userId": "13545254429662867295"
     },
     "user_tz": 420
    },
    "id": "uiPwuDmLIAFk"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    breast_cancer = np.genfromtxt(\"/content/drive/My Drive/CSCI 360 Spring 2021/datasets/breast-cancer-dataset/breast-cancer-wisconsin.data\", delimiter=\",\", dtype=np.float64)\n",
    "except:\n",
    "    breast_cancer = np.genfromtxt(\"./datasets/breast-cancer-dataset/breast-cancer-wisconsin.data\", delimiter=\",\", dtype=np.float64)\n",
    "X = breast_cancer[:,1:-1] #features are the other columns\n",
    "Y = breast_cancer[:,-1] #target value (iris specimen) is last column\n",
    "X = np.nan_to_num(X)\n",
    "Y = np.nan_to_num(Y)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(X)\n",
    "X_train, Y_train, X_test, Y_test = split_data(X, Y)\n",
    "print(X_test.shape)\n",
    "print(X_train.shape)\n",
    "print(Y_test.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation function below copied, as we will use this later on separately in both parts 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(Ypred, Ytest):\n",
    "    accuracy = 0\n",
    "    for predicted, true_val in zip(Ypred, Ytest):\n",
    "        if predicted == true_val:\n",
    "            accuracy += 1\n",
    "    return accuracy / Ytest.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some samples and showing of how an ensemble classifier can increase performance. A standard kNN classifier from the sklearn package cannot break 60% accuracy, but with an ensemble combined with a decision tree, the classifier can break 60% accuracy. Feel free to use the simple ensemble as an example of what you should implement for the 3 ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6207,
     "status": "ok",
     "timestamp": 1617343209504,
     "user": {
      "displayName": "Michael Yuen",
      "photoUrl": "",
      "userId": "13545254429662867295"
     },
     "user_tz": 420
    },
    "id": "1CpUW4e6-5LS",
    "outputId": "143d37c0-f678-4a36-bdbf-caf1afa509f1"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "skkNN = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "skkNN_fit = skkNN.fit(X_train, Y_train)\n",
    "skkNN_pred = skkNN.predict(X_test)\n",
    "eval_results = evaluate(skkNN_pred, Y_test)\n",
    "print(eval_results)\n",
    "\n",
    "simpleEnsemble = VotingClassifier(estimators=[('knn', KNeighborsClassifier(n_neighbors=5)), ('dt', DecisionTreeClassifier(max_depth=5))], voting='soft', weights=[1, 1])\n",
    "simpleEnsemble_fit = simpleEnsemble.fit(X_train, Y_train)\n",
    "simpleEnsemble_pred = simpleEnsemble.predict(X_test)\n",
    "eval_results = evaluate(simpleEnsemble_pred, Y_test)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use some creativity to create 3 different ensembles to break 62% accuracy. Each ensemble classifier should utilize different classifiers from sklearn and/or parameters. The limiting factor here is that you may only use sklearn modules. Using other modules may crash the autograder, hence only use sklearn modules here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, a side note about sklearn: you can find documentation here. https://scikit-learn.org/stable/user_guide.html\n",
    "\n",
    "This is where you can read through the documentation of important pieces of the code and look through tutorials on how to build ensembles with sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 963,
     "status": "ok",
     "timestamp": 1617343210478,
     "user": {
      "displayName": "Michael Yuen",
      "photoUrl": "",
      "userId": "13545254429662867295"
     },
     "user_tz": 420
    },
    "id": "_b3W-bt_MMLN",
    "outputId": "756d0331-7412-4fd8-f545-f1df0ff111b6"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "'''Import all necessary sklearn packages- only use sklearn modules.'''\n",
    "\n",
    "def ensemble_ver_A():\n",
    "    '''Implement this function'''\n",
    "    #pass\n",
    "\n",
    "def ensemble_ver_B():\n",
    "    '''Implement this function'''\n",
    "    #pass\n",
    "\n",
    "def ensemble_ver_C():\n",
    "    '''Implement this function'''\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the results of each ensemble you built below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_A = ensemble_ver_A()\n",
    "ensemble_A_fit = ensemble_A.fit(X_train, Y_train)\n",
    "ensemble_A_pred = ensemble_A.predict(X_test)\n",
    "eval_results = evaluate(ensemble_A_pred, Y_test)\n",
    "print(eval_results)\n",
    "assert eval_results >= 0.62\n",
    "\n",
    "ensemble_B = ensemble_ver_B()\n",
    "ensemble_B_fit = ensemble_B.fit(X_train, Y_train)\n",
    "ensemble_B_pred = ensemble_B.predict(X_test)\n",
    "eval_results = evaluate(ensemble_B_pred, Y_test)\n",
    "print(eval_results)\n",
    "assert eval_results >= 0.62\n",
    "\n",
    "ensemble_C = ensemble_ver_C()\n",
    "ensemble_C_fit = ensemble_C.fit(X_train, Y_train)\n",
    "ensemble_C_pred = ensemble_C.predict(X_test)\n",
    "eval_results = evaluate(ensemble_C_pred, Y_test)\n",
    "print(eval_results)\n",
    "assert eval_results >= 0.62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4558c2356f66e7986e2fc62dc4170e0e",
     "grade": true,
     "grade_id": "9nhei3oru4tb5",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8e8c8a0ee8a1b1050cfb1b6269842e4",
     "grade": true,
     "grade_id": "930hqneor5rjhuwy6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1349d821672cf44e0dd3530911406dae",
     "grade": true,
     "grade_id": "j890i3q4gr5h46trejhu",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyXh7Y28IAFk"
   },
   "source": [
    "### Pytorch and CIFAR-10 setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two cutting edge areas of machine learning include computer vision and natural language processing. Computer vision is the task of having computers understand and analyze image or video data (in an intelligent way, using artificial intelligence). A key task of computer vision is image classification, which we will focus on here. We will take a dataset full of images and classify them into categories, similar to how a human would split images based on what the person sees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is one of two deep learning frameworks that are used in industry and academia consistently (the other being Tensorflow with Keras). We use this framework for neural networks and deep learning in order to have our code run on GPUs with much faster training. When using a framework like PyTorch or TensorFlow, you can harness the power of the GPU for your own custom neural network architectures without having to write CUDA code directly (which is beyond the scope of this class). We want you to be ready to use one of these frameworks for your project so you can experiment more efficiently than if you were writing every feature you want to use by hand. We want you to be exposed to the sort of deep learning code you might run into in academia or industry. Pytorch simplifies the process of creating the neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch documentation can be found here: https://pytorch.org/docs/stable/index.html\n",
    "\n",
    "This is where you can read through the documentation of important pieces of the code and look through tutorials on how to build different network architectures for different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4180,
     "status": "ok",
     "timestamp": 1617343213698,
     "user": {
      "displayName": "Michael Yuen",
      "photoUrl": "",
      "userId": "13545254429662867295"
     },
     "user_tz": 420
    },
    "id": "_WgQh4x_IAFl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cifar-10 is a dataset that is used for image classification. More details can be found here: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "The basic idea is that this dataset has 10 different classes, with 60000 32 by 32 images. 50000 of these are used as training (with 1000 as validation out of that 50000), and 10000 of these are used as testing. \n",
    "\n",
    "The classes are:\n",
    "\n",
    "0. airplane\n",
    "1. automobile\n",
    "2. bird\n",
    "3. cat\n",
    "4. deer\n",
    "5. dog \n",
    "6. frog \n",
    "7. horse \n",
    "8. ship\n",
    "9. truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 135,
     "referenced_widgets": [
      "756d0919b5f049408df71dfefda1ab33",
      "f2c3e632684348f18bd7779872d0aa05",
      "02a3aa990e9849c88cc45fc0642b8303",
      "90b4d3760b3b43e3a99396f8331ac40b",
      "29f5fae704244bf3acabe90bb4886fe9",
      "1fc22d1ede18495eb5d42c0e259f0770",
      "78c9096ea5074cd3bce4366512cbb1db",
      "c4c4263b1fb244e5a98e2722ea040f77"
     ]
    },
    "executionInfo": {
     "elapsed": 13204,
     "status": "ok",
     "timestamp": 1617343222729,
     "user": {
      "displayName": "Michael Yuen",
      "photoUrl": "",
      "userId": "13545254429662867295"
     },
     "user_tz": 420
    },
    "id": "4y4XVp9feb4Z",
    "outputId": "d36fb347-dc71-4a68-89ad-e068cca1e0d4"
   },
   "outputs": [],
   "source": [
    "class ChunkSampler(sampler.Sampler):\n",
    "    \"\"\"Samples elements sequentially from some offset. \n",
    "    Arguments:\n",
    "        num_samples: # of desired datapoints\n",
    "        start: offset where we should start selecting from\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples, start = 0):\n",
    "        self.num_samples = num_samples\n",
    "        self.start = start\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(range(self.start, self.start + self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "NUM_TRAIN = 49000\n",
    "NUM_VAL = 1000\n",
    "\n",
    "try:\n",
    "    cifar10_train = dset.CIFAR10('/content/drive/My Drive/CSCI 360 Spring 2021/datasets/cifar-10', train=True, download=True,\n",
    "                            transform=T.ToTensor())\n",
    "    loader_train = DataLoader(cifar10_train, batch_size=64, sampler=ChunkSampler(NUM_TRAIN, 0))\n",
    "\n",
    "    cifar10_val = dset.CIFAR10('/content/drive/My Drive/CSCI 360 Spring 2021/datasets/cifar-10', train=True, download=True,\n",
    "                            transform=T.ToTensor())\n",
    "    loader_val = DataLoader(cifar10_val, batch_size=64, sampler=ChunkSampler(NUM_VAL, NUM_TRAIN))\n",
    "\n",
    "    cifar10_test = dset.CIFAR10('/content/drive/My Drive/CSCI 360 Spring 2021/datasets/cifar-10', train=False, download=True,\n",
    "                            transform=T.ToTensor())\n",
    "    loader_test = DataLoader(cifar10_test, batch_size=64)\n",
    "except:\n",
    "    cifar10_train = dset.CIFAR10('./datasets/cifar-10', train=True, download=True,\n",
    "                            transform=T.ToTensor())\n",
    "    loader_train = DataLoader(cifar10_train, batch_size=64, sampler=ChunkSampler(NUM_TRAIN, 0))\n",
    "\n",
    "    cifar10_val = dset.CIFAR10('./datasets/cifar-10', train=True, download=True,\n",
    "                            transform=T.ToTensor())\n",
    "    loader_val = DataLoader(cifar10_val, batch_size=64, sampler=ChunkSampler(NUM_VAL, NUM_TRAIN))\n",
    "\n",
    "    cifar10_test = dset.CIFAR10('./datasets/cifar-10', train=False, download=True,\n",
    "                            transform=T.ToTensor())\n",
    "    loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgWBWbbzIAFl"
   },
   "source": [
    "## Question 3) Convolutional Neural Network - 5 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking at a simple model. First, note that PyTorch operates on Tensors, which are n-dimensional arrays functionally analogous to numpy's ndarrays, with the additional feature that they can be used for computations on GPUs.\n",
    "We'll provide you with a Flatten function, which we explain here. Remember that our image data (and more relevantly, our intermediate feature maps) are initially N x C x H x W, where:\n",
    "\n",
    "0. N is the number of datapoints\n",
    "1. C is the number of channels\n",
    "2. H is the height of the intermediate feature map in pixels\n",
    "3. W is the height of the intermediate feature map in pixels\n",
    "\n",
    "This is the right way to represent the data when we are doing something like a 2D convolution, that\n",
    "needs spatial understanding of where the intermediate features are relative to each other. When we input data into fully connected affine layers, however, we want each datapoint to be represented by a single vector -- it's no longer useful to segregate the different channels, rows, and columns of the data. So, we use a \"Flatten\" operation to collapse the C x H x W values per representation into a single long vector. The Flatten function below first reads in the N, C, H, and W values from a given batch of data, and then returns a \"view\" of that data. \"View\" is analogous to numpy's \"reshape\" method: it reshapes x's dimensions to be N x ??, where ?? is allowed to be anything (in this case, it will be C x H x W, but we don't need to specify that explicitly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to training your own model is defining its architecture.\n",
    "\n",
    "Here's an example of a convolutional neural network defined in PyTorch -- try to understand what each line is doing, \n",
    "remembering that each layer is composed upon the previous layer. We haven't trained anything yet - that'll come next- for now, we want you to understand how everything gets set up. nn.Sequential is a container which applies each layer \n",
    "one after the other. \n",
    "\n",
    "In that example, you see 2D convolutional layers (Conv2d), ReLU activations, and fully-connected\n",
    "layers (Linear)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1617343553427,
     "user": {
      "displayName": "Michael Yuen",
      "photoUrl": "",
      "userId": "13545254429662867295"
     },
     "user_tz": 420
    },
    "id": "FifKxMGtU4al",
    "outputId": "9a1e9f96-e0b2-426c-bdf5-aa09540ba00b"
   },
   "outputs": [],
   "source": [
    "# Verify that CUDA is properly configured and you have a GPU available\n",
    "torch.cuda.is_available()\n",
    "\n",
    "import copy\n",
    "dtype = torch.cuda.FloatTensor\n",
    "\n",
    "# This is a little utility that we'll use to reset the model\n",
    "# if we want to re-initialize all our parameters\n",
    "def reset(m):\n",
    "    if hasattr(m, 'reset_parameters'):\n",
    "        m.reset_parameters()\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size() # read in N, C, H, W\n",
    "        return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "\n",
    "# Here's where we define the architecture of the model... \n",
    "simple_model = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, kernel_size=7, stride=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                Flatten(), # see above for explanation\n",
    "                nn.Linear(5408, 10), # affine layer\n",
    "              )\n",
    "\n",
    "# Set the type of all data in this model to be FloatTensor \n",
    "simple_model.type(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure you're doing the right thing, use the following tool to check the dimensionality of your\n",
    "output (it should be 64 x 10, since our batches have size 64 and the output of the final affine layer should be 10, \n",
    "corresponding to our 10 classes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 458,
     "status": "ok",
     "timestamp": 1617343556201,
     "user": {
      "displayName": "Michael Yuen",
      "photoUrl": "",
      "userId": "13545254429662867295"
     },
     "user_tz": 420
    },
    "id": "l9q7t_eEXer4",
    "outputId": "fe745d96-419f-4872-db75-411786cf4b03"
   },
   "outputs": [],
   "source": [
    "## Now we're going to feed a random batch into the model you defined and make sure the output is the right size\n",
    "x = torch.randn(64, 3, 32, 32).type(dtype)\n",
    "x_var = Variable(x.type(dtype)) # Construct a PyTorch Variable out of your input data\n",
    "ans = simple_model(x_var)        # Feed it through the model! \n",
    "\n",
    "# Check to make sure what comes out of your model\n",
    "# is the right dimensionality... this should be True\n",
    "# if you've done everything correctly\n",
    "np.array_equal(np.array(ans.size()), np.array([64, 10]))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch supports many other layer types, loss functions, and optimizers - below we initialize an RMSProp optimizer and calculate the loss with cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1617343558028,
     "user": {
      "displayName": "Michael Yuen",
      "photoUrl": "",
      "userId": "13545254429662867295"
     },
     "user_tz": 420
    },
    "id": "ALsgxpA-IAFm"
   },
   "outputs": [],
   "source": [
    "fixed_model_gpu = copy.deepcopy(simple_model).type(dtype)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(fixed_model_gpu.parameters(), lr=1e-3) # lr sets the learning rate of the optimizer\n",
    "\n",
    "x_gpu = torch.randn(64, 3, 32, 32).type(dtype)\n",
    "x_var_gpu = Variable(x.type(dtype)) # Construct a PyTorch Variable out of your input data\n",
    "ans = fixed_model_gpu(x_var_gpu)        # Feed it through the model! \n",
    "\n",
    "# Check to make sure what comes out of your model\n",
    "# is the right dimensionality... this should be True\n",
    "# if you've done everything correctly\n",
    "np.array_equal(np.array(ans.size()), np.array([64, 10]))\n",
    "\n",
    "ans = simple_model(x_var)\n",
    "\n",
    "torch.cuda.synchronize() # Make sure there are no pending GPU computations\n",
    "ans = fixed_model_gpu(x_var_gpu)        # Feed it through the model! \n",
    "torch.cuda.synchronize() # Make sure there are no pending GPU computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've seen how to define a model, let's walk through how you'd actually train one whole epoch \n",
    "over your training data (using the simple_model we provided above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5690,
     "status": "ok",
     "timestamp": 1617343568399,
     "user": {
      "displayName": "Michael Yuen",
      "photoUrl": "",
      "userId": "13545254429662867295"
     },
     "user_tz": 420
    },
    "id": "3NA6UF8JZTGv",
    "outputId": "01c9e0c1-5cb8-459f-b8e7-e71772e0e807"
   },
   "outputs": [],
   "source": [
    "# This sets the model in \"training\" mode. This is relevant for some layers that may have different behavior\n",
    "# in training mode vs testing mode, such as Dropout and BatchNorm. \n",
    "fixed_model_gpu.train()\n",
    "\n",
    "# Load one batch at a time.\n",
    "for t, (x, y) in enumerate(loader_train):\n",
    "    x_var = Variable(x.type(dtype))\n",
    "    y_var = Variable(y.type(dtype).long())\n",
    "\n",
    "    # This is the forward pass: predict the scores for each class, for each x in the batch.\n",
    "    scores = fixed_model_gpu(x_var)\n",
    "    \n",
    "    # Use the correct y values and the predicted y values to compute the loss.\n",
    "    loss = loss_fn(scores, y_var)\n",
    "    \n",
    "    if (t + 1) % 100 == 0:\n",
    "        print('t = %d, loss = %.4f' % (t + 1, loss.item()))#loss.data[0]))\n",
    "\n",
    "    # Zero out all of the gradients for the variables which the optimizer will update.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # This is the backwards pass: compute the gradient of the loss with respect to each \n",
    "    # parameter of the model.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Actually update the parameters of the model using the gradients computed by the backwards pass.\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you've seen how the training process works in PyTorch. To save you writing boilerplate code,\n",
    "we're providing the following helper functions to help you train for multiple epochs and check the\n",
    "accuracy of your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 282,
     "status": "ok",
     "timestamp": 1617344421398,
     "user": {
      "displayName": "Michael Yuen",
      "photoUrl": "",
      "userId": "13545254429662867295"
     },
     "user_tz": 420
    },
    "id": "OuNYtqNUewwp"
   },
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, num_epochs = 1):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Starting epoch %d / %d' % (epoch + 1, num_epochs))\n",
    "        model.train()\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            x_var = Variable(x.type(dtype))\n",
    "            y_var = Variable(y.type(dtype).long())\n",
    "\n",
    "            scores = model(x_var)\n",
    "            \n",
    "            loss = loss_fn(scores, y_var)\n",
    "            if (t + 1) % 100 == 0:\n",
    "                print('t = %d, loss = %.4f' % (t + 1, loss.item()))#loss.data[0]))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def check_accuracy(model, loader):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    for x, y in loader:\n",
    "        x_var = Variable(x.type(dtype), volatile=True)\n",
    "\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.cpu().max(1)\n",
    "        num_correct += (preds == y).sum()\n",
    "        num_samples += preds.size(0)\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the train and check_accuracy code in action -- feel free to use these methods when\n",
    "evaluating the models you develop below.\n",
    "\n",
    "You should get a training loss of around 1.2-1.4, and a validation accuracy of around 50-60%. As\n",
    "mentioned above, if you re-run the cells, you'll be training more epochs, so your performance will\n",
    "improve past these numbers.\n",
    "\n",
    "But don't worry about getting these numbers better -- this was just practice before you tackle\n",
    "designing your own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15893,
     "status": "ok",
     "timestamp": 1617344443963,
     "user": {
      "displayName": "Michael Yuen",
      "photoUrl": "",
      "userId": "13545254429662867295"
     },
     "user_tz": 420
    },
    "id": "Xe3Ny_s-e2lt",
    "outputId": "4786fca8-cf27-44c5-f6fd-084c667e0eb4"
   },
   "outputs": [],
   "source": [
    "torch.cuda.random.manual_seed(12345)\n",
    "fixed_model_gpu.apply(reset)\n",
    "train(fixed_model_gpu, loss_fn, optimizer, num_epochs=3)\n",
    "check_accuracy(fixed_model_gpu, loader_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a _great_ model on CIFAR-10!\n",
    "\n",
    "Now it's your job to experiment with architectures, hyperparameters, loss functions, and optimizers to train a model that achieves **>=65%** accuracy on the CIFAR-10 **validation** set. You can use the check_accuracy and train functions from above.\n",
    "\n",
    "### Things you should try:\n",
    "- **Filter size**: Above we used 7x7; this makes pretty pictures but smaller filters may be more efficient\n",
    "- **Number of filters**: Above we used 32 filters. Do more or fewer do better?\n",
    "- **Pooling vs Strided Convolution**: Do you use max pooling or just stride convolutions?\n",
    "- **Batch normalization**: Try adding spatial batch normalization after convolution layers and vanilla batch normalization after affine layers. Do your networks train faster?\n",
    "- **Network architecture**: The network above has two layers of trainable parameters. Can you do better with a deep network? Good architectures to try include:\n",
    "    - [conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [conv-relu-conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [batchnorm-relu-conv]xN -> [affine]xM -> [softmax or SVM]\n",
    "- **Global Average Pooling**: Instead of flattening and then having multiple affine layers, perform convolutions until your image gets small (7x7 or so) and then perform an average pooling operation to get to a 1x1 image picture (1, 1 , Filter#), which is then reshaped into a (Filter#) vector. This is used in [Google's Inception Network](https://arxiv.org/abs/1512.00567) (See Table 1 for their architecture).\n",
    "- **Regularization**: Add l2 weight regularization, or perhaps use Dropout.\n",
    "\n",
    "### Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and regularization strength. When doing this there are a couple important things to keep in mind:\n",
    "\n",
    "- If the parameters are working well, you should see improvement within a few hundred iterations\n",
    "- Remember the coarse-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "- You should use the validation set for hyperparameter search, and save your test set for evaluating your architecture on the best parameters as selected by the validation set.\n",
    "\n",
    "### Going above and beyond\n",
    "If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are **not required** to implement any of these; however they would be good things to try.\n",
    "\n",
    "- Alternative update steps: For the assignment we implemented SGD+momentum, RMSprop, and Adam; you could try alternatives like AdaGrad or AdaDelta.\n",
    "- Alternative activation functions such as leaky ReLU, parametric ReLU, ELU, or MaxOut.\n",
    "- Model ensembles\n",
    "- Data augmentation\n",
    "- New Architectures\n",
    "  - [ResNets](https://arxiv.org/abs/1512.03385) where the input from the previous layer is added to the output.\n",
    "  - [DenseNets](https://arxiv.org/abs/1608.06993) where inputs into previous layers are concatenated together.\n",
    "  - [This blog has an in-depth overview](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32)\n",
    "\n",
    "### What we expect\n",
    "At the very least, you should be able to train a ConvNet that gets at least 65% accuracy on the validation set. This is just a lower bound - if you are careful it should be possible to get accuracies much higher than that! \n",
    "\n",
    "You should use the space below to experiment and train your network. \n",
    "\n",
    "Have fun and happy training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 381707,
     "status": "ok",
     "timestamp": 1617344921917,
     "user": {
      "displayName": "Michael Yuen",
      "photoUrl": "",
      "userId": "13545254429662867295"
     },
     "user_tz": 420
    },
    "id": "naLygKi6hccw",
    "outputId": "17bbcef1-a57c-4262-ae74-11dd20329c65"
   },
   "outputs": [],
   "source": [
    "# Train your model here, and make sure the output of this cell is the accuracy of your best model on the \n",
    "# train, val, and test sets. Here's some code to get you started. The output of this cell should be the training\n",
    "# and validation accuracy on your best model (measured by validation accuracy).\n",
    "\n",
    "'''Fill in the model below'''\n",
    "model = nn.Sequential(\n",
    "            )\n",
    "######\n",
    "\n",
    "gpu = copy.deepcopy(model).type(dtype)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#lrates = [1e-2, 3e-2, 5e-2, 7e-2, 1e-3, 3e-3, 5e-3, 7e-3, 1e-4, 3e-4, 5e-4, 7e-4, 1e-5, 3e-5, 5e-5, 7e-5]\n",
    "#for i in lrates:\n",
    "#  optimizer = optim.Adam(gpu.parameters(), lr=i)\n",
    "\n",
    "#  torch.cuda.random.manual_seed(12345)\n",
    "#  gpu.apply(reset)\n",
    "#  train(gpu, loss_fn, optimizer, num_epochs=10)\n",
    "#  check_accuracy(gpu, loader_val)\n",
    "\n",
    "optimizer = optim.Adam(gpu.parameters(), lr=1e-4)\n",
    "\n",
    "torch.cuda.random.manual_seed(12345)\n",
    "gpu.apply(reset)\n",
    "train(gpu, loss_fn, optimizer, num_epochs=50)\n",
    "train_acc = check_accuracy(gpu, loader_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've gotten a result we're happy with, we test our final model on the test set (which you should store in best_model).  This would be the score we would achieve on a competition. Think about how this compares to your validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1712,
     "status": "ok",
     "timestamp": 1617344946061,
     "user": {
      "displayName": "Michael Yuen",
      "photoUrl": "",
      "userId": "13545254429662867295"
     },
     "user_tz": 420
    },
    "id": "ZyEc8chDjc7I",
    "outputId": "11ea7e95-1896-47a7-8ce4-79db208f9189"
   },
   "outputs": [],
   "source": [
    "best_model = gpu\n",
    "test_acc = check_accuracy(best_model, loader_test)\n",
    "assert test_acc >= 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf7959ca7e1c3185f3ab33f0f70c78df",
     "grade": true,
     "grade_id": "89ji3woru4hy98djeirou",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1ztrJIIRX71"
   },
   "source": [
    "### Pytorch and AG News Dataset setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Part 3 for explanation on Pytorch. We will use the torchtext portion to take a deeper look into applying neural networks to natural language processing tasks (focusing on text classification). Natural language processing is the task of analyzing and understanding documents and text through a computer, again using artificial intelligence. Similar to computer vision, we do text classification against a dataset to categorize documents into specific classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AG is a collection of more than 1 million news articles. News articles have been gathered from more than 2000 news sources by ComeToMyHead in more than 1 year of activity. ComeToMyHead is an academic news search engine which has been running since July, 2004. The dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc), information retrieval (ranking, search, etc), xml, data compression, data streaming, and any other non-commercial activity.\n",
    "\n",
    "The AG's news topic classification dataset is constructed by choosing 4 largest classes from the original corpus. Each class contains 30,000 training samples and 1,900 testing samples. The total number of training samples is 120,000 and testing 7,600.\n",
    "\n",
    "The classes are as follows:\n",
    "\n",
    "0. World\n",
    "1. Sports\n",
    "2. Business\n",
    "3. Sci/Tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "try:\n",
    "    import torchtext.datasets as dset\n",
    "    from torchtext.data.utils import get_tokenizer\n",
    "    from collections import Counter\n",
    "    from torchtext.vocab import Vocab\n",
    "except:\n",
    "    import sys\n",
    "    sys.path.append(\"./torchtext\")\n",
    "    import torchtext.datasets as dset\n",
    "    from torchtext.data.utils import get_tokenizer\n",
    "    from collections import Counter\n",
    "    from torchtext.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xlh5DBMRXPZ"
   },
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(torch.device('cuda')), text_list.to(torch.device('cuda')), offsets.to(torch.device('cuda'))\n",
    "\n",
    "try:\n",
    "    ag_train, ag_test = dset.AG_NEWS('/content/drive/My Drive/CSCI 360 Spring 2021/datasets/AG_news')\n",
    "    train_dataset = list(ag_train)\n",
    "    test_dataset = list(ag_test)\n",
    "    num_train = int(len(train_dataset) * 0.95)\n",
    "    split_train, split_valid = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "    dataloader_train = DataLoader(split_train, batch_size=64, shuffle=True, collate_fn=collate_batch)\n",
    "    dataloader_valid = DataLoader(split_valid, batch_size=64, shuffle=True, collate_fn=collate_batch)\n",
    "    dataloader_test = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_batch)\n",
    "    \n",
    "except:\n",
    "    ag_train, ag_test = dset.AG_NEWS('./datasets/AG_news')\n",
    "    train_dataset = list(ag_train)\n",
    "    test_dataset = list(ag_test)\n",
    "    num_train = int(len(train_dataset) * 0.95)\n",
    "    split_train, split_valid = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "    dataloader_train = DataLoader(split_train, batch_size=64, shuffle=True, collate_fn=collate_batch)\n",
    "    dataloader_valid = DataLoader(split_valid, batch_size=64, shuffle=True, collate_fn=collate_batch)\n",
    "    dataloader_test = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMEyErFXIAFm"
   },
   "source": [
    "## Question 4) Recurrent Neural Network - 5 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example for typical NLP data processing with tokenizer and vocabulary. The first step is to build a vocabulary with the raw training dataset. Users can have a customized vocab by setting up arguments in the constructor of the Vocab class. The text pipeline converts a text string into a list of integers based on the lookup table defined in the vocabulary. \n",
    "The label pipeline converts the label into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that CUDA is properly configured and you have a GPU available\n",
    "torch.cuda.is_available()\n",
    "\n",
    "import copy\n",
    "dtype = torch.cuda.FloatTensor\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "counter = Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = Vocab(counter, min_freq=1)\n",
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is composed of the nn.EmbeddingBag layer plus a linear layer for the classification purpose. nn.EmbeddingBag with the default mode of mean computes the mean value of a bag of embeddings. Although the text entries here have different lengths, nn.EmbeddingBag module requires no padding here since the text lengths are saved in offsets.\n",
    "\n",
    "Additionally, since nn.EmbeddingBag accumulates the average across the embeddings on the fly, nn.EmbeddingBag can enhance the performance and memory efficiency to process a sequence of tensors.\n",
    "\n",
    "Besides this, the model consists of a singular linear layer as a simple starting model. Later, you can make a model with more advanced layers, like RNN, LSTM, and GRU layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's where we define the architecture of the model... \n",
    "# We use nn.module instead of sequential, so we have to define both init layers\n",
    "# and the forward function. Defining the forward function allows you to call the\n",
    "# class and run the model step by step.\n",
    "class TextClassificationModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        \n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predited_label = model(text, offsets)\n",
    "        loss = criterion(predited_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predited_label = model(text, offsets)\n",
    "            loss = criterion(predited_label, label)\n",
    "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, training is done using the simple neural network. This should get decent accuracy (~85%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aq2kVTYDIAFn"
   },
   "outputs": [],
   "source": [
    "num_class = len(set([label for (label, text) in train_dataset]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(torch.device('cuda'))\n",
    "\n",
    "EPOCHS = 3 # epoch\n",
    "LR = 5  # learning rate\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, dataloader_train)\n",
    "    accu_val = evaluate(model, dataloader_valid)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Checking the results of test dataset.')\n",
    "accu_test = evaluate(model, dataloader_test)\n",
    "print('test accuracy {:8.3f}'.format(accu_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tips on creating your own RNN model:\n",
    "\n",
    "0. Use different recurrent layers from torchtext and torch.nn, such as nn.RNN, nn.LSTM, and nn.GRU.\n",
    "1. Similar concepts in the CNN can be used in terms of training, optimizers, dropout, layers, and learning rates.\n",
    "2. Feel free to modify the model as you wish- keep in mind the train and evaluate functions will stay the same, but the model can be changed a bit depending on if you come up with a better way to feed the textual data into the forward function.\n",
    "3. You should be able to get above **88%** accuracy.\n",
    "\n",
    "This part is expectedly slightly harder than just using the nn.Sequential framework, as you need to directly deal with the data fed into the model, but the general pattern should stay the same, as we still use Pytorch's framework. The big difference is that you must keep an eye on how the forward function works in tandem of the fed inputs into that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model here, and make sure the output of the cell after this cell is the accuracy of your best model on the \n",
    "# train, val, and test sets. Here's some code to get you started. The output of this cell should be the training\n",
    "# and validation accuracy on your best model (measured by validation accuracy).\n",
    "\n",
    "'''Fill in the model below'''\n",
    "import torch.autograd as autograd\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, text, offsets):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = len(set([label for (label, text) in train_dataset]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "\n",
    "EPOCHS = 15 # epoch\n",
    "LR = 5  # learning rate\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(torch.device('cuda'))\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, dataloader_train)\n",
    "    accu_val = evaluate(model, dataloader_valid)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Checking the results of test dataset.')\n",
    "accu_test = evaluate(model, dataloader_test)\n",
    "print('test accuracy {:8.3f}'.format(accu_test))\n",
    "assert accu_test >= 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f8747474df44cd7e166436435a32d10",
     "grade": true,
     "grade_id": "uih389eorw5ngyuio9k",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope you enjoyed this assignment, this assignment was mostly built from scratch/pieces of other previous homeworks we had experience with or researched online. The material learned in this assignment is relevant to different parts of the data science community. Most likely not all of you will become AI researchers or data scientists, but perhaps little portions of AI can be used in future work, such as implementing simple features in other interdisciplinary fields. For example, if you had built a buy and sell application, using Pytorch to create a machine learning recommendation system is now feasible with the newfound knowledge gained in this project. Hopefully this project was useful in that perspective and gives a introduction to data science topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGaaPkmtIAFn"
   },
   "source": [
    "### Submission: Turn in just this file with your editted code blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "198789f00350b55a7d43d1bebc5ea2e0",
     "grade": true,
     "grade_id": "589jyi34h",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea74f97d397a0b730f9570e396145e2a",
     "grade": true,
     "grade_id": "bhjritu46890mno5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c42fa3585d54ff48aefdd302aac3dc1d",
     "grade": true,
     "grade_id": "0459rt6jkiuwoe",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ea81a50c15235278a2116dc822f1164",
     "grade": true,
     "grade_id": "40jw8i6bhet",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d14587c435a50f01463631b12e31737a",
     "grade": true,
     "grade_id": "4890jweihsbm0e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4558c2356f66e7986e2fc62dc4170e0e",
     "grade": true,
     "grade_id": "9nhei3oru4tb5",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8e8c8a0ee8a1b1050cfb1b6269842e4",
     "grade": true,
     "grade_id": "930hqneor5rjhuwy6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1349d821672cf44e0dd3530911406dae",
     "grade": true,
     "grade_id": "j890i3q4gr5h46trejhu",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf7959ca7e1c3185f3ab33f0f70c78df",
     "grade": true,
     "grade_id": "89ji3woru4hy98djeirou",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f8747474df44cd7e166436435a32d10",
     "grade": true,
     "grade_id": "uih389eorw5ngyuio9k",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Proj4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02a3aa990e9849c88cc45fc0642b8303": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1fc22d1ede18495eb5d42c0e259f0770",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_29f5fae704244bf3acabe90bb4886fe9",
      "value": 170498071
     }
    },
    "1fc22d1ede18495eb5d42c0e259f0770": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29f5fae704244bf3acabe90bb4886fe9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "756d0919b5f049408df71dfefda1ab33": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_02a3aa990e9849c88cc45fc0642b8303",
       "IPY_MODEL_90b4d3760b3b43e3a99396f8331ac40b"
      ],
      "layout": "IPY_MODEL_f2c3e632684348f18bd7779872d0aa05"
     }
    },
    "78c9096ea5074cd3bce4366512cbb1db": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "90b4d3760b3b43e3a99396f8331ac40b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4c4263b1fb244e5a98e2722ea040f77",
      "placeholder": "",
      "style": "IPY_MODEL_78c9096ea5074cd3bce4366512cbb1db",
      "value": " 170499072/? [05:22&lt;00:00, 529092.05it/s]"
     }
    },
    "c4c4263b1fb244e5a98e2722ea040f77": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2c3e632684348f18bd7779872d0aa05": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
